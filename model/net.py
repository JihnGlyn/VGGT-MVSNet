from vggt.models.vggt import VGGT
from vggt.utils.pose_enc import pose_encoding_to_extri_intri

from mvs import *


def run_VGGT(model, images, dtype):
    # images: [B, 3, H, W]
    # hard-coded to use 518 for VGGT
    # images = F.interpolate(images, size=(resolution, resolution), mode="bilinear", align_corners=False)

    with torch.no_grad():
        with torch.cuda.amp.autocast(dtype=dtype):
            images = images[None]  # add batch dimension
            aggregated_tokens_list, ps_idx = model.aggregator(images)

        # Predict Cameras
        pose_enc = model.camera_head(aggregated_tokens_list)[-1]
        # Extrinsic and intrinsic matrices, following OpenCV convention (camera from world)
        extrinsic, intrinsic = pose_encoding_to_extri_intri(pose_enc, images.shape[-2:])
        # Predict Depth Maps
        depth_map, depth_conf, feature = model.depth_head(aggregated_tokens_list, images, ps_idx)

    # extrinsic[B,N,3,4]->[B,N,4,4]
    B, N, _, _ = extrinsic.shape
    last_row = torch.tensor([0, 0, 0, 1], dtype=extrinsic.dtype, device=extrinsic.device).expand(B, N, 1, 4)
    extrinsic = torch.cat((extrinsic, last_row), dim=2)

    # extrinsic[B,N,4,4] intrinsic[B,N,3,3] depth[B,N,H,W] conf[B,N,H,W] feature[B,N,16,H,W]    # TODO: TEST SHAPE
    return extrinsic, intrinsic, depth_map.squeeze(-1), depth_conf, feature


def postprocess_cams(intrinsic, extrinsic, scale: float = 2.0):
    """
    process camera extrinsic and intrinsic generated by VGGT
    :param intrinsic: [B,N,3,3]
    :param extrinsic: [B,N,4,4]
    :param scale: scalar
    :return: proj: [B,N,4,4]
    """
    intrinsic[:, :, :2] *= scale    # upscaled image
    proj = extrinsic.clone()
    proj[:, :, :3, :4] = torch.matmul(intrinsic, extrinsic[:, :, :3, :4])   # TODO: TEST SHAPE
    return proj


def extract_depth_range(depth, conf, threshold):
    """

    :param depth: [B, N, H, W]
    :param conf: [B, N, H, W]
    :param threshold:
    :return: [B] [B]
    """
    mask = conf > threshold
    depth_masked = torch.where(mask, depth, torch.tensor(float('-inf'), dtype=depth.dtype, device=depth.device))
    max_depths = torch.max(depth_masked.view(depth.shape[0], -1), dim=1)[0]

    depth_masked = torch.where(mask, depth, torch.tensor(float('inf'), dtype=depth.dtype, device=depth.device))
    min_depths = torch.min(depth_masked.view(depth.shape[0], -1), dim=1)[0]

    max_depths = torch.where(torch.isinf(max_depths), torch.tensor(0, dtype=depth.dtype, device=depth.device), max_depths)
    min_depths = torch.where(torch.isinf(min_depths), torch.tensor(0, dtype=depth.dtype, device=depth.device), min_depths)

    return max_depths, min_depths


class VGGT4MVS(nn.Module):
    def __init__(self, G=8):
        super(VGGT4MVS, self).__init__()
        self.G = G
        self.mvs = MVSNet(G)
        self.feature_fusion = FeatureFuse(base_channels=16)
        self.refinement = RefineNet(base_channels=8)

    def forward(self, model, imgs, num_depths, depth_interal_ratio, iteration, pair=None):
        # [N] imgs -> [1] ref + [N-1] srcs
        imgs_coarse = imgs["level_2"]
        imgs_mid = imgs["level_1"]
        imgs_fine = imgs["level_0"]
        del imgs
        view_weights = None
        search_ratio = depth_interal_ratio.clone()
        output_depths = []

        # Step 1. Coarse Outputs: VGGT(frozen) -> depth/confidence/intrinsic/extrinsic/features (low-res)
        extrinsic, intrinsic, vggt_depths, vggt_confs, fea_vggt = run_VGGT(model, imgs_coarse, dtype=torch.float32)
        vggt_depths_upscaled = F.interpolate(vggt_depths, scale_factor=2.0, mode='bilinear', align_corners=False)
        depth_min, depth_max = extract_depth_range(vggt_depths_upscaled, vggt_confs, threshold=5)
        B, _, H, W = vggt_depths_upscaled.shape
        
        # Step 2. VGGT to MVS: condition intrinsic/extrinsic -> proj
        proj_matrices = postprocess_cams(intrinsic, extrinsic)
        proj_matrices_4 = postprocess_cams(intrinsic, extrinsic, 4)
        proj_mats = torch.unbind(proj_matrices, dim=1)

        output_projs = [proj_matrices, proj_matrices_4]
        del proj_matrices, proj_matrices_4, intrinsic, extrinsic

        # Step 3. Feature Fusion: upscale depth/features + FPNfeatures -> features
        features = []
        for nview_idx in range(imgs_mid.size()):
            img = imgs_mid[:, nview_idx]
            vggt_fea = fea_vggt[:, nview_idx]
            output_feature = self.feature_fusion(img, vggt_fea)
            features.append(output_feature)

        # Step 4. Combine ref and src views with pair file
        if pair is not None:    # EVALUATION MODE
            _, nviews, _ = pair.shape
            pair_unbind = torch.unbind(pair, dim=1)
            infer_depths = []
            for i in range(nviews):
                ref_proj = proj_mats[i]
                src_projs = [proj_mats[j] for j in pair_unbind[i]]
                ref_fea = features[i]
                src_feas = [features[j] for j in pair_unbind[i]]
                depth_hypo = vggt_depths_upscaled[i].squeeze(0)  # [B,1,H,W] -> [B,H,W]

                # Step 5. Depth Hypothesis and MVS: MVSNet ->depth/confidence (mid-res) # TODO: ITERATIVE AND SHRINK RATIO
                for _ in range(iteration):
                    depth_hypo = get_cur_depth_range_samples(depth_hypo, num_depths, search_ratio)
                    mvsnet_outputs = self.mvs(ref_fea, src_feas, ref_proj, src_projs, depth_hypo, view_weights)
                    search_ratio *= 0.5
                    view_weights = mvsnet_outputs["view_weights"]
                    depth_hypo = mvsnet_outputs["depth"]

                # Step 6. Refinement and Upscale: Refinenet -> depth/confidence (high-res)
                depth_refined = self.refinement(imgs_fine, depth_hypo, depth_min, depth_max)
                infer_depths.append(depth_refined)

            return {"depths": infer_depths}  # List(N)*[B,1,H,W]
        else:   # TRAIN MODE
            ref_proj, src_projs = proj_mats[0], proj_mats[1:]
            ref_fea, src_feas = features[0], features[1:]
            depth_hypo = vggt_depths_upscaled[0].squeeze(0)  # [B,1,H,W] -> [B,H,W]

            # Step 5. Depth Hypothesis and MVS: MVSNet ->depth/confidence (mid-res) # TODO: ITERATIVE AND SHRINK RATIO
            for _ in range(iteration):
                depth_hypo = get_cur_depth_range_samples(depth_hypo, num_depths, search_ratio)
                mvsnet_outputs = self.mvs(ref_fea, src_feas, ref_proj, src_projs, depth_hypo, view_weights)
                search_ratio *= 0.5
                view_weights = mvsnet_outputs["view_weights"]
                depth_hypo = mvsnet_outputs["depth"]
                output_depths.append(depth_hypo)

            # Step 6. Refinement and Upscale: Refinenet -> depth/confidence (high-res)
            depth_refined = self.refinement(imgs_fine, depth_hypo, depth_min, depth_max)
            output_depths.append(depth_refined)

            return {"depth": output_depths, "proj": output_projs}    # List [(iter)*[B,H/2,W/2], [B,1,H,W]]


